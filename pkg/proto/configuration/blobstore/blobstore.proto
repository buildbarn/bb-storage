syntax = "proto3";

package buildbarn.configuration.blobstore;

import "google/rpc/status.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/empty.proto";
import "pkg/proto/configuration/digest/digest.proto";
import "pkg/proto/configuration/grpc/grpc.proto";
import "pkg/proto/configuration/tls/tls.proto";

option go_package = "github.com/buildbarn/bb-storage/pkg/proto/configuration/blobstore";

message AzureBlobAccessConfiguration {
  // Azure storage account name.
  string account_name = 1;

  // The key associated to the account.
  string account_key = 2;

  // The name of the storage container to use.
  string container_name = 3;
}

// Storage configuration for Bazel Buildbarn.
message BlobstoreConfiguration {
  // Storage configuration for the Content Addressable Storage (CAS).
  BlobAccessConfiguration content_addressable_storage = 1;

  // Storage configuration for the Action Cache (AC).
  BlobAccessConfiguration action_cache = 2;
}

message BlobAccessConfiguration {
  oneof backend {
    // Read objects from/write objects to a Redis server.
    RedisBlobAccessConfiguration redis = 2;

    // Read objects from/write objects to a Bazel remote build cache server.
    RemoteBlobAccessConfiguration remote = 3;

    // Cache reads from a slow remote storage backend into a fast
    // local storage backend.
    ReadCachingBlobAccessConfiguration read_caching = 4;

    // Split up objects across two storage backends by digest size.
    SizeDistinguishingBlobAccessConfiguration size_distinguishing = 5;

    // Read objects from/write objects to a circular file on disk.
    // TODO: CircularBlobAccess should be considered deprecated as it is
    // prone to data corruption. LocalBlobAccess should be used instead
    // if at all possible.
    CircularBlobAccessConfiguration circular = 6;

    // Read objects from/write objects to a GRPC service that
    // implements the remote execution protocol.
    buildbarn.configuration.grpc.GRPCClientConfiguration grpc = 7;

    // Always fail with a fixed error response.
    google.rpc.Status error = 8;

    // Fan out requests across multiple storage backends to spread
    // out load.
    ShardingBlobAccessConfiguration sharding = 9;

    // Read objects from/write objects to various cloud-based backends.
    CloudBlobAccessConfiguration cloud = 10;

    // Store blobs in two backends. Blobs present in exactly one backend
    // are automatically replicated to the other backend.
    //
    // This backend does not guarantee high availability, as it does not
    // function in case one backend is unavailable. Crashed backends
    // need to be replaced with functional empty instances. These will
    // be refilled automatically.
    MirroredBlobAccessConfiguration mirrored = 14;

    // Store blobs on the local system.
    //
    // TODO: Right now this backend can only be used to store data in
    // a non-persistent way. We should work towards letting this backend
    // replace circular by supporting persistent on-disk storage.
    LocalBlobAccessConfiguration local = 15;

    // Cache knowledge of which blobs exist locally.
    //
    // Bazel doesn't have a client-side cache with knowledge on which
    // objects are present inside a remote cache. This means that it
    // will often call ContentAddressableStorage.FindMissingBlobs() with
    // sets that have a strong overlap with what was requested
    // previously.
    //
    // This decorator can be used to introduce such a cache server side.
    // It is especially useful for multi-level storage setups. It can
    // cause a reduction in load on storage nodes when this cache
    // enabled on frontend nodes.
    //
    // It only makes sense to use this decorator for the Content
    // Addressable Storage, as FindMissingBlobs() is never called
    // against the Action Cache. The storage backend must also be robust
    // enough to guarantee that objects don't disappear shortly after
    // calling ContentAddressableStorage.FindMissingBlobs(), as that
    // would cause this decorator to cache invalid data.
    ExistenceCachingBlobAccessConfiguration existence_caching = 16;
  }
}

message CircularBlobAccessConfiguration {
  // Directory where the files created by the circular file storage
  // backend are located.
  string directory = 1;

  // Maximum size of the hash table containing data offsets.
  uint64 offset_file_size_bytes = 2;

  // Maximum size of the circular file containing data.
  uint64 data_file_size_bytes = 3;

  // Number of offset entries to cache in memory.
  uint32 offset_cache_size = 4;

  // Instances for which to store entries. For the Content Addressable
  // Storage, this field may be omitted, as data for all instances is
  // stored together. For the Action Cache, this field is required, as
  // every instance needs its own offset file and cache.
  repeated string instances = 5;

  // Amount of space to allocate in the data file at once. Setting
  // this value too low may cause an excessive number of writes to the
  // state file. Setting this value too high may cause excessive
  // amounts of old data to be invalidated upon process restart.
  uint64 data_allocation_chunk_size_bytes = 6;
}

message CloudBlobAccessConfiguration {
  // Prefix for keys, e.g. 'bazel_cas/'.
  string key_prefix = 1;

  // Backend configuration
  oneof config {
    // Url of the bucket to use, currently supports the following schemes:
    // mem, file, s3, gs, azblob.
    string url = 2;
    AzureBlobAccessConfiguration azure = 3;
    GCSBlobAccessConfiguration gcs = 4;
    S3BlobAccessConfiguration s3 = 5;
  }
}

message GCSBlobAccessConfiguration {
  // Name of the bucket to use.
  string bucket = 1;

  // The JWT credentials to authenticate against GCP.
  string credentials = 2;
}

message ReadCachingBlobAccessConfiguration {
  // A remote storage backend that can only be accessed slowly. This
  // storage backend is treated as the source of truth. Write
  // operations are forwarded to this backend.
  BlobAccessConfiguration slow = 1;

  // A local storage backend that can be accessed quickly. This
  // storage backend is treated as a cache. Objects will only be
  // written into it when requested for reading.
  BlobAccessConfiguration fast = 2;
}

message ClusteredRedisBlobAccessConfiguration {
  // Endpoint addresses of the Redis servers.
  repeated string endpoints = 1;

  // Retry configuration, defaults to 16 if not set
  uint32 maximum_retries = 2;

  // After each attempt, the delay will be randomly selected from values
  // between: 0 and min((2^attempt * minimum_retry_backoff),
  // maximum_retry_backoff)
  google.protobuf.Duration minimum_retry_backoff = 3;
  google.protobuf.Duration maximum_retry_backoff = 4;
}

message SingleRedisBlobAccessConfiguration {
  // Endpoint address of the Redis server (e.g., "localhost:6379").
  string endpoint = 1;

  // Numerical ID of the database.
  int32 db = 2;
}

message RedisBlobAccessConfiguration {
  oneof mode {
    // Redis is configured in clustered mode.
    ClusteredRedisBlobAccessConfiguration clustered = 1;

    // Redis is configured as a single server.
    SingleRedisBlobAccessConfiguration single = 2;
  }

  // TLS configuration for the Redis connection. TLS will not be enabled
  // when not set.
  buildbarn.configuration.tls.TLSClientConfiguration tls = 4;

  // How long to keep a cache key in Redis, specified as a duration.
  // When unset, this means keys do not expire and and rely on Redis
  // eviction policy to efficiently remove keys when storage gets full.
  // A reasonable number for this would allow keys to live long enough
  // objects to be found in the CAS once the client uploads them.
  google.protobuf.Duration key_ttl = 7;

  // The minimum number of replicas to successfully replicate put calls to
  // before considering it successful.
  // If unset, no guarantee is made on the number of replicas that contain
  // the contents of a Redis write and master failures can lose data.
  int64 replication_count = 8;

  // The maximum tolerated replication delay expressed in seconds.
  // If unset, Redis write calls return immediately and no attempt is made to
  // ensure that replication succeeds. This can result in data loss when
  // the master is lost.
  google.protobuf.Duration replication_timeout = 9;
}

message RemoteBlobAccessConfiguration {
  // URL of the remote build cache (e.g., "http://localhost:8080/").
  string address = 1;
}

message S3BlobAccessConfiguration {
  // URL of the S3 bucket (e.g., "http://localhost:9000" when using Minio).
  string endpoint = 1;

  // AWS Access Key ID. If unspecified, AWS will search the default credential
  // provider chain.
  string access_key_id = 2;

  // AWS Secret Access Key.
  string secret_access_key = 3;

  // AWS region (e.g., "eu-west-1").
  string region = 4;

  // Whether SSL should be disabled.
  bool disable_ssl = 5;

  // Name of the S3 bucket.
  string bucket = 6;
}

message ShardingBlobAccessConfiguration {
  message Shard {
    // Storage backend that is used by this shard. Omitting this
    // causes the implementation to assume this shard is drained.
    // Requests to this shard will be spread out across the other
    // shards.
    BlobAccessConfiguration backend = 1;

    // Non-zero ratio of how many keys are allocated to this shard.
    // When all shards have equal specifications (i.e., capacity and
    // bandwidth), every shard may have a weight of one.
    //
    // For the backend selection algorithm to run quickly, it is not
    // not advised to let the total weight of drained backends
    // strongly exceed the total weight of undrained ones.
    uint32 weight = 2;
  }

  // Initialization for the hashing algorithm used to partition the
  // key space. This should be a random 64-bit value that is unique to
  // this deployment. Failure to do so may result in poor distribution
  // in case sharding is nested.
  //
  // Changing this value will in effect cause a full repartitioning of
  // the data.
  uint64 hash_initialization = 1;

  // Shards to which requests are routed. To reduce the need for full
  // repartitioning of the data when growing a cluster, it's possible
  // to terminate this list with a drained backend that increases the
  // total weight up to a given number. Newly added backends may
  // allocate their weight from this backend, thereby causing most of
  // the keyspace to still be routed to its original backend.
  repeated Shard shards = 2;
}

message SizeDistinguishingBlobAccessConfiguration {
  // Backend to which to send requests for small blobs (e.g., Redis).
  BlobAccessConfiguration small = 1;

  // Backend to which to send requests for large blobs (e.g., S3).
  BlobAccessConfiguration large = 2;

  // Maximum size of blobs read from/written to the backend for small blobs.
  int64 cutoff_size_bytes = 3;
}

message MirroredBlobAccessConfiguration {
  // Primary backend.
  BlobAccessConfiguration backend_a = 1;

  // Secondary backend.
  BlobAccessConfiguration backend_b = 2;

  // The replication strategy that should be used to copy objects from
  // the primary backend to the secondary backend in case of
  // inconsistencies.
  BlobReplicatorConfiguration replicator_a_to_b = 3;

  // The replication strategy that should be used to copy objects from
  // the secondary backend to the primary backend in case of
  // inconsistencies.
  BlobReplicatorConfiguration replicator_b_to_a = 4;
}

message LocalBlobAccessConfiguration {
  // The digest-location map is a hash table that is used by this
  // storage backend to resolve digests to locations where data is
  // stored. Because entries are small (64 bytes in size), it is
  // recommended to make this map relatively large to reduce collisions.
  //
  // Recommended value: between 2 and 10 times the expected number of
  // objects stored.
  int64 digest_location_map_size = 1;

  // The number of indices a Get() call on the digest-location map may
  // attempt to access. The lower the utilization rate of the
  // digest-location map, the lower this value may be set. For example,
  // if the size of the digest-location map is set in such a way that it
  // is only utilized by 10% (factor 0.1), setting this field to 16
  // means there is only a 0.1^8 chance that inserting an entry
  // prematurely displaces another object from storage.
  //
  // Recommended value: 8
  uint32 digest_location_map_maximum_get_attempts = 2;

  // The number of mutations that a Put() on the digest-location map may
  // perform. Because the digest-location map uses a scheme similar to
  // cuckoo hashing, insertions may cause other entries to be displaced.
  // Those entries may then cause even more entries to be displaced.
  // Because of that, it is recommended to set this field to a small
  // multiple of the maximum Get() attempts.
  //
  // Recommended value: 32
  int64 digest_location_map_maximum_put_attempts = 3;

  // The number of blocks, where attempting to access any data stored
  // within will cause it to be refreshed (i.e., copied into new
  // blocks).
  //
  // Setting the number of old blocks too low may cause builds to fail,
  // due to data disappearing prematurely. Setting the number of old
  // blocks too high may cause an excessive amount of duplication in the
  // data set. For example, if old_blocks == current_blocks + new_blocks,
  // there may be a redundancy in the data set up to a factor of two.
  //
  // Recommended value: 8
  int32 old_blocks = 5;

  // The number of blocks, where attempting to access data stored within
  // will not cause data to be refreshed immediately. The containing
  // block will first need to become old for data to be eligible for
  // refreshes.
  //
  // Recommended value: 24
  int32 current_blocks = 6;

  // The number of blocks where new data needs to be written. It is
  // valid to set this to just 1. Setting it to a slightly higher value
  // has the advantage that frequently used objects will over time get
  // smeared out across the data set. This spreads out the cost
  // refreshing data from old to new blocks.
  //
  // Because the probability of storing objects in new blocks has an
  // inverse exponential distribution, it is not recommended to set this
  // to any value higher than 4. Whereas the first new block will at
  // times be somewhere between 50% and 100% full, the fourth new block
  // will only be between 6.25% and 12.5% full, which is wasteful.
  //
  // Recommended value: 3
  int32 new_blocks = 7;

  // Instances for which to store objects. For the Content Addressable
  // Storage, this field may be omitted, as data for all instances is
  // stored together. For the Action Cache, this field is required, as
  // every instance needs its own digest-location map.
  repeated string instances = 8;

  message InMemory {
    // Data is stored in a list of blocks. The total number of blocks
    // constant over time, with small fluctuations to deal with lingering
    // requests when removing a block. This option sets the size of an
    // individual block.
    //
    // Recommended value: (total space available) /
    //                    (old_blocks + current_blocks + new_blocks)
    int64 block_size_bytes = 1;
  }

  message BlockDevice {
    // Path of the block device where data needs to be stored.
    string path = 1;

    // To deal with lingering read requests, a small number of old
    // blocks may need to be retained for a short period of time. This
    // option determines how many of such blocks are allocated.
    //
    // Unlike in-memory storage, where the block size is configured
    // explicitly, block device backed storage automatically infers an
    // optimal block size. The block size is equal to:
    //
    // block_size = (size of block device) /
    //              (spare_blocks + old_blocks + current_blocks + new_blocks)
    //
    // Recommended value: 3
    int32 spare_blocks = 2;
  }

  oneof data_backend {
    // Store all data in memory.
    InMemory in_memory = 9;

    // Store the blocks containing data directly on a block device. The
    // digest-location map is still stored in memory, meaning that
    // setting this option does not introduce persistency. It is only
    // intended to grow capacity beyond the size of memory.
    BlockDevice block_device = 10;
  }
}

message ExistenceCachingBlobAccessConfiguration {
  // The backend for which results of
  // ContentAddressableStorage.FindMissingBlobs() results need to be
  // cached.
  BlobAccessConfiguration backend = 1;

  // Parameters for the cache data structure that is used by this
  // decorator.
  buildbarn.configuration.digest.ExistenceCacheConfiguration existence_cache =
      2;
}

message BlobReplicatorConfiguration {
  oneof mode {
    // When blobs are only present in one backend, but not the other,
    // they are copied by the client immediately.
    //
    // Because there is no orchestration between clients, this may for
    // certain workloads cause multiple clients to all replicate the
    // same objects. Especially for setups with many clients, this could
    // put a lot of pressure on storage nodes.
    //
    // This strategy may still be acceptable for the Action Cache, even
    // for larger setups. The Action Cache receives less load than the
    // Content Addressable Storage. There is also a lower propbability
    // of clients requesting the same object at around the same time.
    // Action Cache objects also tend to be relatively small, meaning
    // that little space and bandwidth is wasted when replicating
    // objects unnecessarily.
    google.protobuf.Empty local = 1;

    // Instruct an external gRPC service (bb_replicator) to perform
    // replications. This is advised for setups with a larger number of
    // clients, as a centralized replicator process may deduplicate
    // replication actions. This reduces the load on storage nodes.
    //
    // This strategy is only supported for the Content Addressable
    // Storage.
    buildbarn.configuration.grpc.GRPCClientConfiguration remote = 2;

    // Queue and deduplicate all replication operations prior to
    // executing them.
    //
    // In setups with a high volume of requests, it may normally be
    // unsafe to restart a non-persistent storage node. Once the storage
    // node would come back online, it would succumb to traffic
    // generated by clients to replicate missing data.
    //
    // By executing all replication operations sequentially, the amount
    // of pressure placed on storage nodes is bounded. By letting a
    // dedicated bb_replicator instance use this strategy, replication
    // throughput is bounded globally.
    QueuedBlobReplicatorConfiguration queued = 3;
  }
}

message QueuedBlobReplicatorConfiguration {
  // Base replication strategy to which calls should be forwarded.
  BlobReplicatorConfiguration base = 1;

  // Parameters for the cache data structure that is used to incoming
  // deduplicate replication operations.
  buildbarn.configuration.digest.ExistenceCacheConfiguration existence_cache =
      2;
}
