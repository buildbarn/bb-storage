syntax = "proto3";

package buildbarn.configuration.blobstore;

import "google/rpc/status.proto";

option go_package = "github.com/buildbarn/bb-storage/pkg/proto/configuration/blobstore";

message AzureBlobAccessConfiguration {
  // Azure storage account name.
  string account_name = 1;

  // The key associated to the account.
  string account_key = 2;

  // The name of the storage container to use.
  string container_name = 3;
}

// Storage configuration for Bazel Buildbarn.
message BlobstoreConfiguration {
  // Storage configuration for the Content Addressable Storage (CAS).
  BlobAccessConfiguration content_addressable_storage = 1;

  // Storage configuration for the Action Cache (AC).
  BlobAccessConfiguration action_cache = 2;
}

message BlobAccessConfiguration {
  oneof backend {
    // Read objects from/write objects to a Redis server.
    RedisBlobAccessConfiguration redis = 2;

    // Read objects from/write objects to a Bazel remote build cache server.
    RemoteBlobAccessConfiguration remote = 3;

    // Split up objects across two storage backends by digest size.
    SizeDistinguishingBlobAccessConfiguration size_distinguishing = 5;

    // Read objects from/write objects to a circular file on disk.
    CircularBlobAccessConfiguration circular = 6;

    // Read objects from/write objects to a GRPC service that
    // implements the remote execution protocol.
    GRPCBlobAccessConfiguration grpc = 7;

    // Always fail with a fixed error response.
    google.rpc.Status error = 8;

    // Fan out requests across multiple storage backends to spread
    // out load.
    ShardingBlobAccessConfiguration sharding = 9;

    // Read objects from/write objects to various cloud-based backends.
    CloudBlobAccessConfiguration cloud = 10;
  }
}

message CircularBlobAccessConfiguration {
  // Directory where the files created by the circular file storage
  // backend are located.
  string directory = 1;

  // Maximum size of the hash table containing data offsets.
  uint64 offset_file_size_bytes = 2;

  // Maximum size of the circular file containing data.
  uint64 data_file_size_bytes = 3;

  // Number of offset entries to cache in memory.
  uint32 offset_cache_size = 4;

  // Instances for which to store entries. For the Content Addressable
  // Storage, this field may be omitted, as data for all instances is
  // stored together. For the Action Cache, this field is required, as
  // every instance needs its own offset file and cache.
  repeated string instance = 5;

  // Amount of space to allocate in the data file at once. Setting
  // this value too low may cause an excessive number of writes to the
  // state file. Setting this value too high may cause excessive
  // amounts of old data to be invalidated upon process restart.
  uint64 data_allocation_chunk_size_bytes = 6;
}

message CloudBlobAccessConfiguration {
  // Prefix for keys, e.g. 'bazel_cas/'.
  string key_prefix = 1;

  // Backend configuration
  oneof config {
    // Url of the bucket to use, currently supports the following schemes:
    // mem, file, s3, gs, azblob.
    string url = 2;
    AzureBlobAccessConfiguration azure = 3;
    GCSBlobAccessConfiguration gcs = 4;
    S3BlobAccessConfiguration s3 = 5;
  }
}

message GCSBlobAccessConfiguration {
  // Name of the bucket to use.
  string bucket = 1;

  // The JWT credentials to authenticate against GCP.
  string credentials = 2;
}

message GRPCBlobAccessConfiguration {
  // Endpoint address of the GRPC server (e.g., "localhost:8982").
  string endpoint = 1;
}

message RedisBlobAccessConfiguration {
  // Endpoint address of the Redis server (e.g., "localhost:6379").
  string endpoint = 1;

  // Numerical ID of the database.
  int32 db = 2;
}

message RemoteBlobAccessConfiguration {
  // URL of the remote build cache (e.g., "http://localhost:8080/").
  string address = 1;
}

message S3BlobAccessConfiguration {
  // URL of the S3 bucket (e.g., "http://localhost:9000" when using Minio).
  string endpoint = 1;

  // AWS Access Key ID. If unspecified, AWS will search the default credential provider chain.
  string access_key_id = 2;

  // AWS Secret Access Key.
  string secret_access_key = 3;

  // AWS region (e.g., "eu-west-1").
  string region = 4;

  // Whether SSL should be disabled.
  bool disable_ssl = 5;

  // Name of the S3 bucket.
  string bucket = 6;
}

message ShardingBlobAccessConfiguration {
  message Shard {
    // Storage backend that is used by this shard. Omitting this
    // causes the implementation to assume this shard is drained.
    // Requests to this shard will be spread out across the other
    // shards.
    BlobAccessConfiguration backend = 1;

    // Non-zero ratio of how many keys are allocated to this shard.
    // When all shards have equal specifications (i.e., capacity and
    // bandwidth), every shard may have a weight of one.
    //
    // For the backend selection algorithm to run quickly, it is not
    // not advised to let the total weight of drained backends
    // strongly exceed the total weight of undrained ones.
    uint32 weight = 2;
  }

  // Initialization for the hashing algorithm used to partition the
  // key space. This should be a random 64-bit value that is unique to
  // this deployment. Failure to do so may result in poor distribution
  // in case sharding is nested.
  //
  // Changing this value will in effect cause a full repartitioning of
  // the data.
  uint64 hash_initialization = 1;

  // Shards to which requests are routed. To reduce the need for full
  // repartitioning of the data when growing a cluster, it's possible
  // to terminate this list with a drained backend that increases the
  // total weight up to a given number. Newly added backends may
  // allocate their weight from this backend, thereby causing most of
  // the keyspace to still be routed to its original backend.
  repeated Shard shard = 2;
}

message SizeDistinguishingBlobAccessConfiguration {
  // Backend to which to send requests for small blobs (e.g., Redis).
  BlobAccessConfiguration small = 1;

  // Backend to which to send requests for large blobs (e.g., S3).
  BlobAccessConfiguration large = 2;

  // Maximum size of blobs read from/written to the backend for small blobs.
  int64 cutoff_size_bytes = 3;
}
